{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNokoPg2+vyE7gt9ddKStTo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **I. Dataset Preparation**\n","\n","These procedures are implemented to prepare the COPIOUS Corpus dataset for relabion labeling. The currec format of `.txt` and `.ann` files does not support relation annotations."],"metadata":{"id":"53rawI77lJzW"}},{"cell_type":"markdown","source":["## **I-A. Mounting the COPIOUS Dataset**\n"],"metadata":{"id":"Ia_Qs15QlAV4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ry_hGK5SVuQf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731901435226,"user_tz":-480,"elapsed":44433,"user":{"displayName":"Jeezer Niño Lee","userId":"05141950893678382408"}},"outputId":"0877cf27-b1c9-4a1d-a46b-9d457985cd4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# folder path\n","folder = \"/content/drive/My Drive/CS_198/copious_published/copious_published/\"\n","subfolders = [\"train\",\"dev\",\"test\"]"]},{"cell_type":"markdown","source":["## **I-B. Extracting the Data**\n","\n","These lines of code here aims to initially split the full raw text per file into sentences, and only those sentences where entities as labeled in the `.ann` file are preserved in a dataframe and exported into Google Sheets."],"metadata":{"id":"tT3NIc86lHRw"}},{"cell_type":"code","source":["## Single sentence\n","import os\n","import pandas as pd\n","import nltk\n","\n","\n","nltk.download('punkt_tab')\n","\n","# Initialize storage for all entities\n","all_entities = []\n","\n","# Function to find the sentence containing the entity\n","def find_sentence(text, start, end):\n","    sentences = nltk.sent_tokenize(text)\n","    for sentence in sentences:\n","        if text[start:end] in sentence:\n","            return sentence.strip()\n","    return \"No Sentence\"\n","\n","folder_path = \"/content/drive/My Drive/CS_198/copious_published/copious_published/train\"\n","# Process each .ann and corresponding .txt file\n","for file_name in os.listdir(folder_path):\n","    if file_name.endswith(\".ann\"):\n","        base_name = file_name[:-4]  # Remove .ann extension\n","        txt_file_path = os.path.join(folder_path, base_name + \".txt\")\n","        ann_file_path = os.path.join(folder_path, file_name)\n","\n","        with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n","            text = txt_file.read()\n","\n","        with open(ann_file_path, 'r', encoding='utf-8') as ann_file:\n","            annotations = ann_file.readlines()\n","\n","        # Parse entities\n","        entities = []\n","        for annotation in annotations:\n","            if annotation.startswith('T'):\n","                parts = annotation.split('\\t')\n","                entity_id, entity_info, entity_text = parts\n","                entity_type, spans = entity_info.split(' ', 1)\n","\n","                span_text = []\n","                span_ranges = spans.split(';')\n","                for span_range in span_ranges:\n","                    start, end = map(int, span_range.split())\n","                    span_text.append(text[start:end])\n","\n","                combined_text = \" \".join(span_text)\n","\n","                first_span_start, first_span_end = map(int, span_ranges[0].split())\n","                sentence = find_sentence(text, first_span_start, first_span_end)\n","\n","                entities.append({\n","                    \"File\": base_name,\n","                    \"Entity ID\": entity_id,\n","                    \"Entity Type\": entity_type,\n","                    \"Spans\": spans,\n","                    \"Text\": combined_text.strip(),\n","                    \"Sentence\": sentence\n","                })\n","\n","        # Store for all files\n","        all_entities.extend(entities)\n","\n","# Create a DataFrame for entities\n","entity_df = pd.DataFrame(all_entities)\n","\n","# Save to Excel in Google Drive\n","output_path = os.path.join(\"/content/drive/My Drive/CS_198/copious_published/\", \"sentence_entities_summary.xlsx\")\n","entity_df.to_excel(output_path, index=False)\n","\n","entity_df.head(10)\n","\n","print(f\"Done: {output_path}!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfjumpCAmbUX","executionInfo":{"status":"ok","timestamp":1731902062265,"user_tz":-480,"elapsed":97198,"user":{"displayName":"Jeezer Niño Lee","userId":"05141950893678382408"}},"outputId":"d5f4366b-3db5-4eed-87c4-b74a71c1e0ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Done: /content/drive/My Drive/CS_198/copious_published/sentence_entities_summary.xlsx!\n"]}]},{"cell_type":"markdown","source":["## **I.C. Multi-Sentence Chunking**\n","\n","Given the observation from the `.xlxs` file earlier, We decided to redo the preprocessing, where the consecutive and previous sentences are to be included into one long sequence together with the specific sentence of intertest to capture two entities in a single sentence.\n","\n","Note that identitcal rows are dropped to avoid redundancy."],"metadata":{"id":"D5-d59voWj_J"}},{"cell_type":"code","source":["# Multi Sentence (#CHUNK SIZE 3)\n","import os\n","import pandas as pd\n","import nltk\n","\n","nltk.download('punkt_tab')\n","\n","folder = \"/content/drive/My Drive/CS_198/copious_published/copious_published/\"\n","subfolders = [\"train\",\"dev\",\"test\"]\n","\n","# Function to find the sentence containing the entity\n","def chunk_sentences(text, chunk_size=3):\n","    \"\"\"Split text into overlapping chunks of sentences.\"\"\"\n","    sentences = nltk.sent_tokenize(text)\n","    chunks = []\n","    for i in range(len(sentences) - chunk_size + 1):\n","        chunks.append(\" \".join(sentences[i:i + chunk_size]))\n","    return chunks\n","\n","\n","for subfolder in subfolders:\n","\n","  # list for all entitites\n","  all_entities = []\n","\n","  folder_path = os.path.join(folder, subfolder)\n","  for file_name in os.listdir(folder_path):\n","      if file_name.endswith(\".ann\"):\n","          base_name = file_name[:-4]\n","          txt_file_path = os.path.join(folder_path, base_name + \".txt\")\n","          ann_file_path = os.path.join(folder_path, file_name)\n","\n","          with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n","              text = txt_file.read()\n","\n","          with open(ann_file_path, 'r', encoding='utf-8') as ann_file:\n","              annotations = ann_file.readlines()\n","\n","          # Tokenize text into chunks of 3 sentences\n","          sentence_chunks = chunk_sentences(text, chunk_size=3)\n","\n","          # Parse entities\n","          entities_per_chunk = []\n","          for annotation in annotations:\n","              if annotation.startswith('T'):\n","                  parts = annotation.split('\\t')\n","                  entity_id, entity_info, entity_text = parts\n","                  entity_type, spans = entity_info.split(' ', 1)\n","\n","                  span_text = []\n","                  span_ranges = spans.split(';')\n","                  for span_range in span_ranges:\n","                      start, end = map(int, span_range.split())\n","                      span_text.append(text[start:end])\n","\n","                  combined_text = \" \".join(span_text)\n","\n","                  first_span_start, first_span_end = map(int, span_ranges[0].split())\n","\n","                  # Check which chunk has the entity\n","                  for chunk in sentence_chunks:\n","                      if combined_text in chunk:\n","                          entities_per_chunk.append({\n","                              \"File\": base_name,\n","                              \"Chunk\": chunk,\n","                              \"Entity ID\": entity_id,\n","                              \"Entity Type\": entity_type,\n","                              \"Entity Text\": combined_text.strip(),\n","                              \"Spans\": spans\n","                          })\n","                          break\n","\n","          # Store entities for this specfic file\n","          all_entities.extend(entities_per_chunk)\n","\n","  entity_df = pd.DataFrame(all_entities)\n","  entity_df = entity_df.drop_duplicates()\n","\n","  # Save to drive as .xlsx\n","  output_path = os.path.join(\"/content/drive/My Drive/CS_198/copious_published/\", f\"{subfolder}_entities_summary.xlsx\")\n","  #output_path = os.path.join(\"/content/drive/My Drive/CS_198/copious_published/\", \"chunked_entities_summary.xlsx\")\n","  entity_df.to_excel(output_path, index=False)\n","\n","  print(f\"Chunked entity annotations exported to {output_path}!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lqWze0KZLdEo","executionInfo":{"status":"ok","timestamp":1731890881126,"user_tz":-480,"elapsed":63901,"user":{"displayName":"Jeezer Niño Lee","userId":"05141950893678382408"}},"outputId":"2519f9cd-d3a0-42b1-e3fa-f3ee1f6dc939"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Chunked entity annotations exported to /content/drive/My Drive/CS_198/copious_published/train_entities_summary.xlsx!\n","Chunked entity annotations exported to /content/drive/My Drive/CS_198/copious_published/dev_entities_summary.xlsx!\n","Chunked entity annotations exported to /content/drive/My Drive/CS_198/copious_published/test_entities_summary.xlsx!\n"]}]},{"cell_type":"markdown","source":["## **I.D. Assessing the number of samples available**\n","\n","These summaries reveal that the entity types themselves are imbalanced, which might further affect the results later."],"metadata":{"id":"djBSmAVIXd0P"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# folder and subfolders paths\n","folder = \"/content/drive/My Drive/CS_198/copious_published/\"\n","subfolders = [\"train\", \"dev\", \"test\"]\n","\n","final_summary = pd.DataFrame()\n","\n","for subfolder in subfolders:\n","    # Load xlsx\n","    entity_file_path = os.path.join(folder, f\"{subfolder}_entities_summary.xlsx\")\n","    entity_df = pd.read_excel(entity_file_path)\n","\n","    # Count each type occurences\n","    entity_type_counts = entity_df[\"Entity Type\"].value_counts().reset_index()\n","    entity_type_counts.columns = [\"Entity Type\", \"Count\"]\n","    entity_type_counts[\"Subfolder\"] = subfolder\n","\n","    # Append to the final summary df\n","    final_summary = pd.concat([final_summary, entity_type_counts], ignore_index=True)\n","\n","\n","pivot_summary = final_summary.pivot(index=\"Entity Type\", columns=\"Subfolder\", values=\"Count\").fillna(0)\n","\n","pivot_output_path = os.path.join(folder, \"entity_type_occurrences_summary.xlsx\")\n","pivot_summary.to_excel(pivot_output_path)\n","\n","print(pivot_summary)\n","\n","print(f\"Entity type occurrences summary exported to {pivot_output_path}!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vnC6dY0qtWF","executionInfo":{"status":"ok","timestamp":1731890889952,"user_tz":-480,"elapsed":6412,"user":{"displayName":"Jeezer Niño Lee","userId":"05141950893678382408"}},"outputId":"80c84fce-421b-44b6-e8e8-d432629168d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Subfolder              dev  test  train\n","Entity Type                            \n","GeographicalLocation   992   871   7883\n","Habitat                 89   153   1258\n","Person                 180   230   2413\n","Taxon                 1546  1320   9319\n","TemporalExpression     157   251   1737\n","Entity type occurrences summary exported to /content/drive/My Drive/CS_198/copious_published/entity_type_occurrences_summary.xlsx!\n"]}]},{"cell_type":"markdown","source":["## **I-E. Relation Pairings**\n","\n","For this section, the whole dataset is further processed, where rows with the same chunks but varying entity text and types are merged into one row, increasing the total columns of the dataset while decreasing the rows in the process.\n","\n","This format makes it easier for annotators reading the chunk of text, as the pair of entities occuring in the said chunk would be easily indentifiable and oughts to bring convenience to the annotation process.\n","\n","The final resulting dataframes are then exported to their corresponding `.xlsx` files."],"metadata":{"id":"3VSkuafPbeQl"}},{"cell_type":"code","source":["folder = \"/content/drive/My Drive/CS_198/copious_published/\"\n","subfolders = [\"train\",\"dev\",\"test\"]\n","\n","for subfolder in subfolders:\n","  output_path = os.path.join(folder, f\"{subfolder}_entities_summary.xlsx\")\n","\n","  # Load generated .xlsx file\n","  annotations_path = output_path\n","  entity_df = pd.read_excel(annotations_path)\n","\n","  relation_rows = []\n","\n","  # Iterate through each files in the df\n","  for file_name in entity_df[\"File\"].unique():\n","      # Filter per file\n","      file_entities = entity_df[entity_df[\"File\"] == file_name]\n","\n","      # Group by chunk\n","      grouped_chunks = file_entities.groupby(\"Chunk\")\n","\n","      for chunk, entities_in_chunk in grouped_chunks:\n","          # Find all combinations of desired entities\n","          for i, entity_1 in entities_in_chunk.iterrows():\n","              for j, entity_2 in entities_in_chunk.iterrows():\n","                  # entity_1 != entity_2\n","                  if i < j:\n","                      # Check for the desired pairings\n","                      if (\n","                          (entity_1[\"Entity Type\"] == \"GeographicalLocation\" and entity_2[\"Entity Type\"] == \"Taxon\")\n","                          or (entity_1[\"Entity Type\"] == \"GeographicalLocation\" and entity_2[\"Entity Type\"] == \"Habitat\")\n","                      ):\n","                          # Append the relationship row to the list\n","                          relation_rows.append({\n","                              \"File\": file_name,\n","                              \"Entity 1\": entity_1[\"Entity Text\"],\n","                              \"Entity 1 Type\": entity_1[\"Entity Type\"],\n","                              \"Entity 2\": entity_2[\"Entity Text\"],\n","                              \"Entity 2 Type\": entity_2[\"Entity Type\"],\n","                              \"Chunk\": chunk\n","                          })\n","\n","  # Create a new df for the relations\n","  relations_df = pd.DataFrame(relation_rows)\n","  relations_df = relations_df.drop_duplicates()\n","\n","  # Save to Excel in Drive\n","  output_path_relations = os.path.join(\"/content/drive/My Drive/CS_198/copious_published/\", f\"{subfolder}_relations_summary.xlsx\")\n","  #output_path_relations = os.path.join(\"/content/drive/My Drive/CS_198/copious_published/\", \"relations_summary.xlsx\")\n","  relations_df.to_excel(output_path_relations, index=False)\n","\n","  print(f\"Relations exported to {output_path_relations}!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVh-MWMKbmEe","executionInfo":{"status":"ok","timestamp":1731890944069,"user_tz":-480,"elapsed":25417,"user":{"displayName":"Jeezer Niño Lee","userId":"05141950893678382408"}},"outputId":"074a4459-af76-44ea-83be-07e39649593b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Relations exported to /content/drive/My Drive/CS_198/copious_published/train_relations_summary.xlsx!\n","Relations exported to /content/drive/My Drive/CS_198/copious_published/dev_relations_summary.xlsx!\n","Relations exported to /content/drive/My Drive/CS_198/copious_published/test_relations_summary.xlsx!\n"]}]},{"cell_type":"markdown","source":["## **I.F. Summary of Entity-Pairs**\n","\n","The code below simply counts the number of desired entity pairs occuring with each division of the dataset. As one would see, the amount of samples seem to be quite small, which might prove to be problematic during model training and fine-tuning."],"metadata":{"id":"mXhkZcBGZgdt"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# folder and subfolders paths\n","folder = \"/content/drive/My Drive/CS_198/copious_published/\"\n","subfolders = [\"train\", \"dev\", \"test\"]\n","\n","summary_counts = []\n","\n","for subfolder in subfolders:\n","    # Load xlsx file\n","    relations_path = os.path.join(folder, f\"{subfolder}_relations_summary.xlsx\")\n","    relations_df = pd.read_excel(relations_path)\n","\n","    # Count Geolocation-Taxon and Geolocation-Habitat relations\n","    geolocation_taxon_count = relations_df[\n","        (relations_df[\"Entity 1 Type\"] == \"GeographicalLocation\") &\n","        (relations_df[\"Entity 2 Type\"] == \"Taxon\")\n","    ].shape[0]\n","\n","    geolocation_habitat_count = relations_df[\n","        (relations_df[\"Entity 1 Type\"] == \"GeographicalLocation\") &\n","        (relations_df[\"Entity 2 Type\"] == \"Habitat\")\n","    ].shape[0]\n","\n","    # Append counts to the summary structure\n","    summary_counts.append({\n","        \"Subfolder\": subfolder,\n","        \"Geolocation-Taxon\": geolocation_taxon_count,\n","        \"Geolocation-Habitat\": geolocation_habitat_count\n","    })\n","\n","# Convert summary to a df and store as xlsx file\n","summary_df = pd.DataFrame(summary_counts)\n","relations_path = os.path.join(\"/content/drive/My Drive/CS_198/copious_published/\", \"desired_relations_summary.xlsx\")\n","summary_df.to_excel(relations_path, index=False)\n","\n","print(summary_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFjjMMF8pDsP","executionInfo":{"status":"ok","timestamp":1731890949085,"user_tz":-480,"elapsed":1056,"user":{"displayName":"Jeezer Niño Lee","userId":"05141950893678382408"}},"outputId":"6bd75cb7-399d-466a-c5ce-25c93ca071b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Subfolder  Geolocation-Taxon  Geolocation-Habitat\n","0     train               2437                  691\n","1       dev                106                   30\n","2      test                313                   64\n"]}]},{"cell_type":"markdown","source":["Further discussion of these are in the Connference Paper Draft, where afterwards the researcher has proceeded with relation labeling given that the dataset has been preprocessed in a format that allows annotations."],"metadata":{"id":"C3QXWfqCZ4tc"}}]}